{
  "posted_total": 1,
  "posted_today": [
    "2505.21497"
  ],
  "posted_papers": [
    "2505.21497"
  ],
  "queue": [
    {
      "arxiv_id": "2505.21488",
      "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation",
      "authors": [
        "Omer Dahary",
        "Yehonathan Cohen",
        "Or Patashnik",
        "Kfir Aberman",
        "Daniel Cohen-Or"
      ],
      "abstract": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "published": "2025-05-27T17:54:24+00:00",
      "updated": "2025-05-27T17:54:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21488v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21488v1",
      "doi": null,
      "altmetric_score": 9.95,
      "altmetric_data": {
        "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation",
        "arxiv_id": "2505.21488",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 13,
          "sci": 3
        },
        "context": {
          "all": {
            "count": 28359361,
            "mean": 10.859106004468863,
            "rank": 4007802,
            "pct": 85,
            "higher_than": 24345603
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730809477622286,
            "rank": 68697,
            "pct": 94,
            "higher_than": 1085834
          },
          "similar_age_3m": {
            "count": 144531,
            "mean": 9.784819990175116,
            "rank": 17533,
            "pct": 87,
            "higher_than": 127025
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0386988784990527,
            "rank": 807,
            "pct": 97,
            "higher_than": 32450
          }
        },
        "authors": [
          "Omer Dahary",
          "Yehonathan Cohen",
          "Or Patashnik",
          "Kfir Aberman",
          "Daniel Cohen-Or"
        ],
        "type": "article",
        "pubdate": 1748368464,
        "altmetric_id": 177463743,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 5,
        "cited_by_posts_count": 23,
        "cited_by_rdts_count": 1,
        "cited_by_tweeters_count": 11,
        "cited_by_accounts_count": 17,
        "last_updated": 1748456313,
        "score": 9.95,
        "history": {
          "1y": 9.95,
          "6m": 9.95,
          "3m": 9.95,
          "1m": 9.95,
          "1w": 9.95,
          "6d": 9.95,
          "5d": 9.95,
          "4d": 9.95,
          "3d": 9.95,
          "2d": 9.95,
          "1d": 9.95,
          "at": 9.95
        },
        "url": "https://arxiv.org/abs/2505.21488",
        "added_on": 1748410599,
        "published_on": 1748368464,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=10&types=rtttttuu",
          "medium": "https://badges.altmetric.com/?size=100&score=10&types=rtttttuu",
          "large": "https://badges.altmetric.com/?size=180&score=10&types=rtttttuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463743"
      },
      "priority_score": 211.05747197430554
    },
    {
      "arxiv_id": "2505.21411",
      "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
      "authors": [
        "Yehui Tang",
        "Xiaosong Li",
        "Fangcheng Liu",
        "Wei Guo",
        "Hang Zhou",
        "Yaoyuan Wang",
        "Kai Han",
        "Xianzhi Yu",
        "Jinpeng Li",
        "Hui Zang",
        "Fei Mi",
        "Xiaojun Meng",
        "Zhicheng Liu",
        "Hanting Chen",
        "Binfan Zheng",
        "Can Chen",
        "Youliang Yan",
        "Ruiming Tang",
        "Peifeng Qin",
        "Xinghao Chen",
        "Dacheng Tao",
        "Yunhe Wang"
      ],
      "abstract": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-05-27T16:40:21+00:00",
      "updated": "2025-05-27T16:40:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21411v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21411v1",
      "doi": null,
      "altmetric_score": 9.5,
      "altmetric_data": {
        "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
        "arxiv_id": "2505.21411",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 2
        },
        "context": {
          "all": {
            "count": 28355400,
            "mean": 10.859259895399115,
            "rank": 4107242,
            "pct": 85,
            "higher_than": 24190655
          },
          "journal": {
            "count": 1154716,
            "mean": 3.972587737590889,
            "rank": 71770,
            "pct": 93,
            "higher_than": 1082203
          },
          "similar_age_3m": {
            "count": 141963,
            "mean": 9.80582665905905,
            "rank": 17668,
            "pct": 87,
            "higher_than": 124412
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0067063351273338,
            "rank": 838,
            "pct": 97,
            "higher_than": 32398
          }
        },
        "authors": [
          "Yehui Tang",
          "Xiaosong Li",
          "Fangcheng Liu",
          "Wei Guo",
          "Hang Zhou",
          "Yaoyuan Wang",
          "Kai Han",
          "Xianzhi Yu",
          "Jinpeng Li",
          "Hui Zang",
          "Fei Mi",
          "Xiaojun Meng",
          "Zhicheng Liu",
          "Hanting Chen",
          "Binfan Zheng",
          "Can Chen",
          "Youliang Yan",
          "Ruiming Tang",
          "Peifeng Qin",
          "Xinghao Chen",
          "Dacheng Tao",
          "Yunhe Wang"
        ],
        "type": "article",
        "pubdate": 1748364021,
        "altmetric_id": 177463442,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 2,
        "cited_by_posts_count": 11,
        "cited_by_msm_count": 1,
        "cited_by_accounts_count": 3,
        "last_updated": 1748420972,
        "score": 9.5,
        "history": {
          "1y": 9.5,
          "6m": 9.5,
          "3m": 9.5,
          "1m": 9.5,
          "1w": 9.5,
          "6d": 9.5,
          "5d": 9.5,
          "4d": 9.5,
          "3d": 9.5,
          "2d": 9.5,
          "1d": 9.5,
          "at": 9.5
        },
        "url": "https://arxiv.org/abs/2505.21411",
        "added_on": 1748409278,
        "published_on": 1748364021,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=10&types=mmuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=10&types=mmuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=10&types=mmuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463442"
      },
      "priority_score": 181.2878886397222
    },
    {
      "arxiv_id": "2505.21493",
      "title": "Reinforcing General Reasoning without Verifiers",
      "authors": [
        "Xiangxin Zhou",
        "Zichen Liu",
        "Anya Sims",
        "Haonan Wang",
        "Tianyu Pang",
        "Chongxuan Li",
        "Liang Wang",
        "Min Lin",
        "Chao Du"
      ],
      "abstract": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-05-27T17:56:27+00:00",
      "updated": "2025-05-27T17:56:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21493v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21493v1",
      "doi": null,
      "altmetric_score": 6.25,
      "altmetric_data": {
        "title": "Reinforcing General Reasoning without Verifiers",
        "arxiv_id": "2505.21493",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 11,
          "sci": 1
        },
        "context": {
          "all": {
            "count": 28357944,
            "mean": 10.858551910603959,
            "rank": 6107410,
            "pct": 78,
            "higher_than": 22209348
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730777435222397,
            "rank": 110549,
            "pct": 90,
            "higher_than": 1044125
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.759528248922075,
            "rank": 28859,
            "pct": 79,
            "higher_than": 114428
          },
          "similar_age_journal_3m": {
            "count": 33256,
            "mean": 2.0332732739956696,
            "rank": 1267,
            "pct": 96,
            "higher_than": 31978
          }
        },
        "authors": [
          "Xiangxin Zhou",
          "Zichen Liu",
          "Anya Sims",
          "Haonan Wang",
          "Tianyu Pang",
          "Chongxuan Li",
          "Liang Wang",
          "Min Lin",
          "Chao Du"
        ],
        "type": "article",
        "pubdate": 1748368587,
        "altmetric_id": 177460617,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_posts_count": 21,
        "cited_by_tweeters_count": 10,
        "cited_by_bluesky_count": 2,
        "cited_by_fbwalls_count": 1,
        "cited_by_accounts_count": 13,
        "last_updated": 1748447086,
        "score": 6.25,
        "history": {
          "1y": 6.25,
          "6m": 6.25,
          "3m": 6.25,
          "1m": 6.25,
          "1w": 6.25,
          "6d": 6.25,
          "5d": 6.25,
          "4d": 6.25,
          "3d": 6.25,
          "2d": 6.25,
          "1d": 6.25,
          "at": 6.25
        },
        "url": "https://arxiv.org/abs/2505.21493",
        "added_on": 1748398594,
        "published_on": 1748368587,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=7&types=tttttfuu",
          "medium": "https://badges.altmetric.com/?size=100&score=7&types=tttttfuu",
          "large": "https://badges.altmetric.com/?size=180&score=7&types=tttttfuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177460617"
      },
      "priority_score": 141.2345553061111
    },
    {
      "arxiv_id": "2505.21451",
      "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of\n  Violent Communication",
      "authors": [
        "Jocelyn Shen",
        "Akhila Yerukola",
        "Xuhui Zhou",
        "Cynthia Breazeal",
        "Maarten Sap",
        "Hae Won Park"
      ],
      "abstract": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-05-27T17:23:57+00:00",
      "updated": "2025-05-27T17:23:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21451v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21451v1",
      "doi": null,
      "altmetric_score": 3.1,
      "altmetric_data": {
        "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication",
        "arxiv_id": "2505.21451",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "sci": 2,
          "pub": 4
        },
        "context": {
          "all": {
            "count": 28359361,
            "mean": 10.859105762926044,
            "rank": 9649494,
            "pct": 65,
            "higher_than": 18657342
          },
          "journal": {
            "count": 1154720,
            "mean": 3.973075015588195,
            "rank": 179382,
            "pct": 84,
            "higher_than": 972702
          },
          "similar_age_3m": {
            "count": 144531,
            "mean": 9.784772595498541,
            "rank": 40593,
            "pct": 71,
            "higher_than": 102901
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.036932439339728,
            "rank": 2515,
            "pct": 92,
            "higher_than": 30685
          }
        },
        "authors": [
          "Jocelyn Shen",
          "Akhila Yerukola",
          "Xuhui Zhou",
          "Cynthia Breazeal",
          "Maarten Sap",
          "Hae Won Park"
        ],
        "type": "article",
        "pubdate": 1748366637,
        "altmetric_id": 177463410,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 2,
        "cited_by_posts_count": 14,
        "cited_by_tweeters_count": 4,
        "cited_by_accounts_count": 6,
        "last_updated": 1748455747,
        "score": 3.1,
        "history": {
          "1y": 3.1,
          "6m": 3.1,
          "3m": 3.1,
          "1m": 3.1,
          "1w": 3.1,
          "6d": 3.1,
          "5d": 3.1,
          "4d": 3.1,
          "3d": 3.1,
          "2d": 3.1,
          "1d": 3.1,
          "at": 3.1
        },
        "url": "https://arxiv.org/abs/2505.21451",
        "added_on": 1748409187,
        "published_on": 1748366637,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=4&types=ttttuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=4&types=ttttuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=4&types=ttttuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463410"
      },
      "priority_score": 44.21122197236111
    },
    {
      "arxiv_id": "2505.21439",
      "title": "Towards Better Instruction Following Retrieval Models",
      "authors": [
        "Yuchen Zhuang",
        "Aaron Trinh",
        "Rushi Qiang",
        "Haotian Sun",
        "Chao Zhang",
        "Hanjun Dai",
        "Bo Dai"
      ],
      "abstract": "Modern information retrieval (IR) models, trained exclusively on standard\n<query, passage> pairs, struggle to effectively interpret and follow explicit\nuser instructions. We introduce InF-IR, a large-scale, high-quality training\ncorpus tailored for enhancing retrieval models in Instruction-Following IR.\nInF-IR expands traditional training pairs into over 38,000 expressive\n<instruction, query, passage> triplets as positive samples. In particular, for\neach positive triplet, we generate two additional hard negative examples by\npoisoning both instructions and queries, then rigorously validated by an\nadvanced reasoning model (o3-mini) to ensure semantic plausibility while\nmaintaining instructional incorrectness. Unlike existing corpora that primarily\nsupport computationally intensive reranking tasks for decoder-only language\nmodels, the highly contrastive positive-negative triplets in InF-IR further\nenable efficient representation learning for smaller encoder-only models,\nfacilitating direct embedding-based retrieval. Using this corpus, we train\nInF-Embed, an instruction-aware Embedding model optimized through contrastive\nlearning and instruction-query attention mechanisms to align retrieval outcomes\nprecisely with user intents. Extensive experiments across five\ninstruction-based retrieval benchmarks demonstrate that InF-Embed significantly\nsurpasses competitive baselines by 8.1% in p-MRR, measuring the\ninstruction-following capabilities.",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-05-27T17:14:37+00:00",
      "updated": "2025-05-27T17:14:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21439v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21439v1",
      "doi": null,
      "altmetric_score": 2.6,
      "altmetric_data": {
        "title": "Towards Better Instruction Following Retrieval Models",
        "arxiv_id": "2505.21439",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 6,
          "sci": 1
        },
        "context": {
          "all": {
            "count": 28355400,
            "mean": 10.85925965205922,
            "rank": 16635613,
            "pct": 41,
            "higher_than": 11660195
          },
          "journal": {
            "count": 1154716,
            "mean": 3.972581762095616,
            "rank": 209004,
            "pct": 81,
            "higher_than": 942626
          },
          "similar_age_3m": {
            "count": 141963,
            "mean": 9.805778054845277,
            "rank": 44976,
            "pct": 68,
            "higher_than": 96936
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0054841095643283,
            "rank": 3059,
            "pct": 90,
            "higher_than": 30110
          }
        },
        "authors": [
          "Yuchen Zhuang",
          "Aaron Trinh",
          "Rushi Qiang",
          "Haotian Sun",
          "Chao Zhang",
          "Hanjun Dai",
          "Bo Dai"
        ],
        "type": "article",
        "pubdate": 1748366077,
        "altmetric_id": 177463421,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 4,
        "cited_by_posts_count": 13,
        "cited_by_tweeters_count": 3,
        "cited_by_accounts_count": 7,
        "last_updated": 1748420626,
        "score": 2.6,
        "history": {
          "1y": 2.6,
          "6m": 2.6,
          "3m": 2.6,
          "1m": 2.6,
          "1w": 2.6,
          "6d": 2.6,
          "5d": 2.6,
          "4d": 2.6,
          "3d": 2.6,
          "2d": 2.6,
          "1d": 2.6,
          "at": 2.6
        },
        "url": "https://arxiv.org/abs/2505.21439",
        "added_on": 1748409218,
        "published_on": 1748366077,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=3&types=tttuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=3&types=tttuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=3&types=tttuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463421"
      },
      "priority_score": 38.78344419430556
    },
    {
      "arxiv_id": "2505.21505",
      "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
      "authors": [
        "Shimao Zhang",
        "Zhejian Lai",
        "Xiang Liu",
        "Shuaijie She",
        "Xiao Liu",
        "Yeyun Gong",
        "Shujian Huang",
        "Jiajun Chen"
      ],
      "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-05-27T17:59:52+00:00",
      "updated": "2025-05-27T17:59:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21505v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21505v1",
      "doi": null,
      "altmetric_score": 2.5,
      "altmetric_data": {
        "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective",
        "arxiv_id": "2505.21505",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 6
        },
        "context": {
          "all": {
            "count": 28359361,
            "mean": 10.859105741769008,
            "rank": 16698144,
            "pct": 40,
            "higher_than": 11473295
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730744959817104,
            "rank": 212147,
            "pct": 80,
            "higher_than": 929753
          },
          "similar_age_3m": {
            "count": 144531,
            "mean": 9.78476844414001,
            "rank": 45857,
            "pct": 66,
            "higher_than": 96780
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0362348837908537,
            "rank": 3196,
            "pct": 89,
            "higher_than": 29759
          }
        },
        "authors": [
          "Shimao Zhang",
          "Zhejian Lai",
          "Xiang Liu",
          "Shuaijie She",
          "Xiao Liu",
          "Yeyun Gong",
          "Shujian Huang",
          "Jiajun Chen"
        ],
        "type": "article",
        "pubdate": 1748368792,
        "altmetric_id": 177461614,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_posts_count": 13,
        "cited_by_tweeters_count": 3,
        "cited_by_bluesky_count": 3,
        "cited_by_accounts_count": 6,
        "last_updated": 1748455393,
        "score": 2.5,
        "history": {
          "1y": 2.5,
          "6m": 2.5,
          "3m": 2.5,
          "1m": 2.5,
          "1w": 2.5,
          "6d": 2.5,
          "5d": 2.5,
          "4d": 2.5,
          "3d": 2.5,
          "2d": 2.5,
          "1d": 2.5,
          "at": 2.5
        },
        "url": "https://arxiv.org/abs/2505.21505",
        "added_on": 1748401523,
        "published_on": 1748368792,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=3&types=tttttuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=3&types=tttttuuu",
          "large": "https://badges.altmetric.com/?size=180&score=3&types=tttttuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177461614"
      },
      "priority_score": 38.45052752736111
    },
    {
      "arxiv_id": "2505.21460",
      "title": "High-Dimensional Calibration from Swap Regret",
      "authors": [
        "Maxwell Fishelson",
        "Noah Golowich",
        "Mehryar Mohri",
        "Jon Schneider"
      ],
      "abstract": "We study the online calibration of multi-dimensional forecasts over an\narbitrary convex set $\\mathcal{P} \\subset \\mathbb{R}^d$ relative to an\narbitrary norm $\\Vert\\cdot\\Vert$. We connect this with the problem of external\nregret minimization for online linear optimization, showing that if it is\npossible to guarantee $O(\\sqrt{\\rho T})$ worst-case regret after $T$ rounds\nwhen actions are drawn from $\\mathcal{P}$ and losses are drawn from the dual\n$\\Vert \\cdot \\Vert_*$ unit norm ball, then it is also possible to obtain\n$\\epsilon$-calibrated forecasts after $T = \\exp(O(\\rho /\\epsilon^2))$ rounds.\nWhen $\\mathcal{P}$ is the $d$-dimensional simplex and $\\Vert \\cdot \\Vert$ is\nthe $\\ell_1$-norm, the existence of $O(\\sqrt{T\\log d})$-regret algorithms for\nlearning with experts implies that it is possible to obtain\n$\\epsilon$-calibrated forecasts after $T = \\exp(O(\\log{d}/\\epsilon^2)) =\nd^{O(1/\\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).\n  Interestingly, our algorithm obtains this guarantee without requiring access\nto any online linear optimization subroutine or knowledge of the optimal rate\n$\\rho$ -- in fact, our algorithm is identical for every setting of\n$\\mathcal{P}$ and $\\Vert \\cdot \\Vert$. Instead, we show that the optimal\nregularizer for the above OLO problem can be used to upper bound the above\ncalibration error by a swap regret, which we then minimize by running the\nrecent TreeSwap algorithm with Follow-The-Leader as a subroutine.\n  Finally, we prove that any online calibration algorithm that guarantees\n$\\epsilon T$ $\\ell_1$-calibration error over the $d$-dimensional simplex\nrequires $T \\geq \\exp(\\mathrm{poly}(1/\\epsilon))$ (assuming $d \\geq\n\\mathrm{poly}(1/\\epsilon)$). This strengthens the corresponding\n$d^{\\Omega(\\log{1/\\epsilon})}$ lower bound of Peng, and shows that an\nexponential dependence on $1/\\epsilon$ is necessary.",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.GT",
        "stat.ML"
      ],
      "published": "2025-05-27T17:31:47+00:00",
      "updated": "2025-05-27T17:31:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21460v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21460v1",
      "doi": null,
      "altmetric_score": 2,
      "altmetric_data": {
        "title": "High-Dimensional Calibration from Swap Regret",
        "arxiv_id": "2505.21460",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 7,
          "sci": 1
        },
        "context": {
          "all": {
            "count": 28357944,
            "mean": 10.858551760734137,
            "rank": 17293936,
            "pct": 37,
            "higher_than": 10687576
          },
          "journal": {
            "count": 1154716,
            "mean": 3.972581242487331,
            "rank": 249724,
            "pct": 76,
            "higher_than": 880812
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.759498645194094,
            "rank": 50206,
            "pct": 62,
            "higher_than": 89909
          },
          "similar_age_journal_3m": {
            "count": 33255,
            "mean": 2.0200819726356936,
            "rank": 4200,
            "pct": 84,
            "higher_than": 28227
          }
        },
        "authors": [
          "Maxwell Fishelson",
          "Noah Golowich",
          "Mehryar Mohri",
          "Jon Schneider"
        ],
        "type": "article",
        "pubdate": 1748367107,
        "altmetric_id": 177460370,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 5,
        "cited_by_posts_count": 16,
        "cited_by_tweeters_count": 3,
        "cited_by_accounts_count": 8,
        "last_updated": 1748436084,
        "score": 2,
        "history": {
          "1y": 2,
          "6m": 2,
          "3m": 2,
          "1m": 2,
          "1w": 2,
          "6d": 2,
          "5d": 2,
          "4d": 2,
          "3d": 2,
          "2d": 2,
          "1d": 2,
          "at": 2
        },
        "url": "https://arxiv.org/abs/2505.21460",
        "added_on": 1748397384,
        "published_on": 1748367107,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=tttuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=tttuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=tttuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177460370"
      },
      "priority_score": 36.966499749166665
    },
    {
      "arxiv_id": "2505.21459",
      "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics",
      "authors": [
        "Xiangru Jian",
        "Wei Pang",
        "Zhengyuan Dong",
        "Chao Zhang",
        "M. Tamer \u00d6zsu"
      ],
      "abstract": "Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale.",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.MM"
      ],
      "published": "2025-05-27T17:31:17+00:00",
      "updated": "2025-05-27T17:31:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21459v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21459v1",
      "doi": null,
      "altmetric_score": 1.5,
      "altmetric_data": {
        "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics",
        "arxiv_id": "2505.21459",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 6
        },
        "context": {
          "all": {
            "count": 28355400,
            "mean": 10.859259613265905,
            "rank": 18227167,
            "pct": 33,
            "higher_than": 9536932
          },
          "journal": {
            "count": 1154716,
            "mean": 3.9725808094804274,
            "rank": 313596,
            "pct": 68,
            "higher_than": 794193
          },
          "similar_age_3m": {
            "count": 141963,
            "mean": 9.805770306347428,
            "rank": 58478,
            "pct": 55,
            "higher_than": 78738
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0044188339998197,
            "rank": 6576,
            "pct": 73,
            "higher_than": 24463
          }
        },
        "authors": [
          "Xiangru Jian",
          "Wei Pang",
          "Zhengyuan Dong",
          "Chao Zhang",
          "M. Tamer \u00d6zsu"
        ],
        "type": "article",
        "pubdate": 1748367077,
        "altmetric_id": 177463898,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 5,
        "cited_by_posts_count": 11,
        "cited_by_tweeters_count": 1,
        "cited_by_accounts_count": 6,
        "last_updated": 1748419723,
        "score": 1.5,
        "history": {
          "1y": 1.5,
          "6m": 1.5,
          "3m": 1.5,
          "1m": 1.5,
          "1w": 1.5,
          "6d": 1.5,
          "5d": 1.5,
          "4d": 1.5,
          "3d": 1.5,
          "2d": 1.5,
          "1d": 1.5,
          "at": 1.5
        },
        "url": "https://arxiv.org/abs/2505.21459",
        "added_on": 1748411743,
        "published_on": 1748367077,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=ttuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=ttuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=ttuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463898"
      },
      "priority_score": 34.212333081527774
    },
    {
      "arxiv_id": "2505.21445",
      "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale\n  Longitudinal Dataset in English and Mandarin",
      "authors": [
        "Zhiqi Ai",
        "Meixuan Bao",
        "Zhiyong Chen",
        "Zhi Yang",
        "Xinnuo Li",
        "Shugong Xu"
      ],
      "abstract": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research.",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-05-27T17:16:59+00:00",
      "updated": "2025-05-27T17:16:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21445v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21445v1",
      "doi": null,
      "altmetric_score": 1.5,
      "altmetric_data": {
        "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin",
        "arxiv_id": "2505.21445",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 6
        },
        "context": {
          "all": {
            "count": 28359361,
            "mean": 10.859105706507282,
            "rank": 18227112,
            "pct": 33,
            "higher_than": 9541210
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730736299709024,
            "rank": 313688,
            "pct": 68,
            "higher_than": 795035
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.759495162402567,
            "rank": 59030,
            "pct": 55,
            "higher_than": 79157
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0349961213506123,
            "rank": 6708,
            "pct": 73,
            "higher_than": 24296
          }
        },
        "authors": [
          "Zhiqi Ai",
          "Meixuan Bao",
          "Zhiyong Chen",
          "Zhi Yang",
          "Xinnuo Li",
          "Shugong Xu"
        ],
        "type": "article",
        "pubdate": 1748366219,
        "altmetric_id": 177464231,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 5,
        "cited_by_posts_count": 10,
        "cited_by_tweeters_count": 1,
        "cited_by_accounts_count": 6,
        "last_updated": 1748453588,
        "score": 1.5,
        "history": {
          "1y": 1.5,
          "6m": 1.5,
          "3m": 1.5,
          "1m": 1.5,
          "1w": 1.5,
          "6d": 1.5,
          "5d": 1.5,
          "4d": 1.5,
          "3d": 1.5,
          "2d": 1.5,
          "1d": 1.5,
          "at": 1.5
        },
        "url": "https://arxiv.org/abs/2505.21445",
        "added_on": 1748412147,
        "published_on": 1748366219,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=ttuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=ttuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=ttuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177464231"
      },
      "priority_score": 34.09316641458334
    },
    {
      "arxiv_id": "2505.21491",
      "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
      "authors": [
        "Boyang Wang",
        "Xuweiyi Chen",
        "Matheus Gadelha",
        "Zezhou Cheng"
      ],
      "abstract": "Controllability, temporal coherence, and detail synthesis remain the most\ncritical challenges in video generation. In this paper, we focus on a commonly\nused yet underexplored cinematic technique known as Frame In and Frame Out.\nSpecifically, starting from image-to-video generation, users can control the\nobjects in the image to naturally leave the scene or provide breaking new\nidentity references to enter the scene, guided by user-specified motion\ntrajectory. To support this task, we introduce a new dataset curated\nsemi-automatically, a comprehensive evaluation protocol targeting this setting,\nand an efficient identity-preserving motion-controllable video Diffusion\nTransformer architecture. Our evaluation shows that our proposed approach\nsignificantly outperforms existing baselines.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-05-27T17:56:07+00:00",
      "updated": "2025-05-27T17:56:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21491v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21491v1",
      "doi": null,
      "altmetric_score": 1.75,
      "altmetric_data": {
        "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
        "arxiv_id": "2505.21491",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "sci": 1,
          "pub": 2
        },
        "context": {
          "all": {
            "count": 28357944,
            "mean": 10.858551751918267,
            "rank": 17907768,
            "pct": 35,
            "higher_than": 10200716
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730738464736044,
            "rank": 277827,
            "pct": 73,
            "higher_than": 844398
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.75949690379833,
            "rank": 56084,
            "pct": 59,
            "higher_than": 85434
          },
          "similar_age_journal_3m": {
            "count": 33255,
            "mean": 2.0302437528191244,
            "rank": 5201,
            "pct": 80,
            "higher_than": 26699
          }
        },
        "authors": [
          "Boyang Wang",
          "Xuweiyi Chen",
          "Matheus Gadelha",
          "Zezhou Cheng"
        ],
        "type": "article",
        "pubdate": 1748368567,
        "altmetric_id": 177462152,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_posts_count": 8,
        "cited_by_tweeters_count": 1,
        "cited_by_bluesky_count": 2,
        "cited_by_rdts_count": 1,
        "cited_by_accounts_count": 4,
        "last_updated": 1748413154,
        "score": 1.75,
        "history": {
          "1y": 1.75,
          "6m": 1.75,
          "3m": 1.75,
          "1m": 1.75,
          "1w": 1.75,
          "6d": 1.75,
          "5d": 1.75,
          "4d": 1.75,
          "3d": 1.75,
          "2d": 1.75,
          "1d": 1.75,
          "at": 1.75
        },
        "url": "https://arxiv.org/abs/2505.21491",
        "added_on": 1748402910,
        "published_on": 1748368567,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=rrtttuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=rrtttuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=rrtttuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177462152"
      },
      "priority_score": 33.73177752541667
    },
    {
      "arxiv_id": "2505.21441",
      "title": "Autoencoding Random Forests",
      "authors": [
        "Binh Duc Vu",
        "Jan Kapar",
        "Marvin Wright",
        "David S. Watson"
      ],
      "abstract": "We propose a principled method for autoencoding with random forests. Our\nstrategy builds on foundational results from nonparametric statistics and\nspectral graph theory to learn a low-dimensional embedding of the model that\noptimally represents relationships in the data. We provide exact and\napproximate solutions to the decoding problem via constrained optimization,\nsplit relabeling, and nearest neighbors regression. These methods effectively\ninvert the compression pipeline, establishing a map from the embedding space\nback to the input space using splits learned by the ensemble's constituent\ntrees. The resulting decoders are universally consistent under common\nregularity assumptions. The procedure works with supervised or unsupervised\nmodels, providing a window into conditional or joint distributions. We\ndemonstrate various applications of this autoencoder, including powerful new\ntools for visualization, compression, clustering, and denoising. Experiments\nillustrate the ease and utility of our method in a wide range of settings,\nincluding tabular, image, and genomic data.",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-05-27T17:15:02+00:00",
      "updated": "2025-05-27T17:15:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21441v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21441v1",
      "doi": null,
      "altmetric_score": 1.25,
      "altmetric_data": {
        "title": "Autoencoding Random Forests",
        "arxiv_id": "2505.21441",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 5
        },
        "context": {
          "all": {
            "count": 28359361,
            "mean": 10.859105697691852,
            "rank": 18967418,
            "pct": 31,
            "higher_than": 8848948
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730734134682004,
            "rank": 362910,
            "pct": 63,
            "higher_than": 731243
          },
          "similar_age_3m": {
            "count": 144531,
            "mean": 9.7847597954764,
            "rank": 65706,
            "pct": 49,
            "higher_than": 71816
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0390912534952945,
            "rank": 9071,
            "pct": 62,
            "higher_than": 20691
          }
        },
        "authors": [
          "Binh Duc Vu",
          "Jan Kapar",
          "Marvin Wright",
          "David S. Watson"
        ],
        "type": "article",
        "pubdate": 1748366102,
        "altmetric_id": 177462786,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 4,
        "cited_by_posts_count": 10,
        "cited_by_tweeters_count": 1,
        "cited_by_accounts_count": 5,
        "last_updated": 1748457270,
        "score": 1.25,
        "history": {
          "1y": 1.25,
          "6m": 1.25,
          "3m": 1.25,
          "1m": 1.25,
          "1w": 1.25,
          "6d": 1.25,
          "5d": 1.25,
          "4d": 1.25,
          "3d": 1.25,
          "2d": 1.25,
          "1d": 1.25,
          "at": 1.25
        },
        "url": "https://arxiv.org/abs/2505.21441",
        "added_on": 1748405091,
        "published_on": 1748366102,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=ttuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=ttuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=ttuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177462786"
      },
      "priority_score": 32.88941641402778
    },
    {
      "arxiv_id": "2505.21479",
      "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?",
      "authors": [
        "Keenan Samway",
        "Max Kleiman-Weiner",
        "David Guzman Piedrahita",
        "Rada Mihalcea",
        "Bernhard Sch\u00f6lkopf",
        "Zhijing Jin"
      ],
      "abstract": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-05-27T17:51:18+00:00",
      "updated": "2025-05-27T17:51:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21479v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21479v1",
      "doi": null,
      "altmetric_score": 1.85,
      "altmetric_data": {
        "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?",
        "doi": "10.48550/arxiv.2505.21479",
        "arxiv_id": "2505.21479",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 4
        },
        "context": {
          "all": {
            "count": 28357944,
            "mean": 10.858551755444616,
            "rank": 17673824,
            "pct": 36,
            "higher_than": 10449991
          },
          "journal": {
            "count": 1154716,
            "mean": 3.97258111258526,
            "rank": 274019,
            "pct": 75,
            "higher_than": 876901
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.759497600356637,
            "rank": 53397,
            "pct": 60,
            "higher_than": 87523
          },
          "similar_age_journal_3m": {
            "count": 33255,
            "mean": 2.0220155164636897,
            "rank": 5038,
            "pct": 84,
            "higher_than": 28085
          }
        },
        "authors": [
          "Keenan Samway",
          "Max Kleiman-Weiner",
          "David Guzman Piedrahita",
          "Rada Mihalcea",
          "Bernhard Sch\u00f6lkopf",
          "Zhijing Jin"
        ],
        "type": "article",
        "pubdate": 1748368278,
        "altmetric_id": 177463279,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 2,
        "cited_by_posts_count": 10,
        "cited_by_tweeters_count": 2,
        "cited_by_accounts_count": 4,
        "last_updated": 1748437883,
        "score": 1.85,
        "history": {
          "1y": 1.85,
          "6m": 1.85,
          "3m": 1.85,
          "1m": 1.85,
          "1w": 1.85,
          "6d": 1.85,
          "5d": 1.85,
          "4d": 1.85,
          "3d": 1.85,
          "2d": 1.85,
          "1d": 1.85,
          "at": 1.85
        },
        "url": "http://dx.doi.org/10.48550/arxiv.2505.21479",
        "added_on": 1748407987,
        "published_on": 1748368278,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=tttuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=tttuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=tttuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463279"
      },
      "priority_score": 32.75163863597223
    },
    {
      "arxiv_id": "2505.21419",
      "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG\n  LLMs",
      "authors": [
        "Yifan Wang",
        "Kenneth P. Birman"
      ],
      "abstract": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.",
      "categories": [
        "cs.AI",
        "cs.OS"
      ],
      "published": "2025-05-27T16:43:45+00:00",
      "updated": "2025-05-27T16:43:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21419v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21419v1",
      "doi": "10.1145/3721146.3721958",
      "altmetric_score": 1,
      "altmetric_data": {
        "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs",
        "doi": "10.1145/3721146.3721958",
        "arxiv_id": "2505.21419",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 3
        },
        "context": {
          "all": {
            "count": 28355400,
            "mean": 10.85925959563258,
            "rank": 19560730,
            "pct": 21,
            "higher_than": 5982734
          },
          "journal": {
            "count": 1154716,
            "mean": 3.9725803764735237,
            "rank": 425560,
            "pct": 55,
            "higher_than": 639716
          },
          "similar_age_3m": {
            "count": 316800,
            "mean": 10.643522285353534,
            "rank": 165015,
            "pct": 34,
            "higher_than": 109642
          },
          "similar_age_journal_3m": {
            "count": 65748,
            "mean": 2.2583035833789618,
            "rank": 25042,
            "pct": 46,
            "higher_than": 30622
          }
        },
        "authors": [
          "Yifan Wang",
          "Kenneth P. Birman"
        ],
        "type": "article",
        "pubdate": 1743292800,
        "epubdate": 1743465600,
        "dimensions_publication_id": "pub.1187185291",
        "altmetric_id": 177464156,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 2,
        "cited_by_posts_count": 6,
        "cited_by_tweeters_count": 1,
        "cited_by_accounts_count": 3,
        "last_updated": 1748419549,
        "score": 1,
        "history": {
          "1y": 1,
          "6m": 1,
          "3m": 1,
          "1m": 1,
          "1w": 1,
          "6d": 1,
          "5d": 1,
          "4d": 1,
          "3d": 1,
          "2d": 1,
          "1d": 1,
          "at": 1
        },
        "url": "http://dx.doi.org/10.1145/3721146.3721958",
        "added_on": 1748412080,
        "published_on": 1743465600,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=1&types=tttuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=1&types=tttuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=1&types=tttuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177464156"
      },
      "priority_score": 31.56622196902778
    },
    {
      "arxiv_id": "2505.21444",
      "title": "Can Large Reasoning Models Self-Train?",
      "authors": [
        "Sheikh Shafayat",
        "Fahim Tajwar",
        "Ruslan Salakhutdinov",
        "Jeff Schneider",
        "Andrea Zanette"
      ],
      "abstract": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-05-27T17:16:00+00:00",
      "updated": "2025-05-27T17:16:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21444v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21444v1",
      "doi": null,
      "altmetric_score": 1.25,
      "altmetric_data": {
        "title": "Can Large Reasoning Models Self-Train?",
        "arxiv_id": "2505.21444",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "sci": 1,
          "pub": 1
        },
        "context": {
          "all": {
            "count": 28359361,
            "mean": 10.859105697691852,
            "rank": 18967418,
            "pct": 31,
            "higher_than": 8848948
          },
          "journal": {
            "count": 1154720,
            "mean": 3.9730734134682004,
            "rank": 362910,
            "pct": 63,
            "higher_than": 731243
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.759493421006804,
            "rank": 65706,
            "pct": 50,
            "higher_than": 71816
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 2.0351284163685017,
            "rank": 9055,
            "pct": 62,
            "higher_than": 20712
          }
        },
        "authors": [
          "Sheikh Shafayat",
          "Fahim Tajwar",
          "Ruslan Salakhutdinov",
          "Jeff Schneider",
          "Andrea Zanette"
        ],
        "type": "article",
        "pubdate": 1748366160,
        "altmetric_id": 177464844,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 1,
        "cited_by_posts_count": 8,
        "cited_by_tweeters_count": 1,
        "cited_by_accounts_count": 2,
        "last_updated": 1748453738,
        "score": 1.25,
        "history": {
          "1y": 1.25,
          "6m": 1.25,
          "3m": 1.25,
          "1m": 1.25,
          "1w": 1.25,
          "6d": 1.25,
          "5d": 1.25,
          "4d": 1.25,
          "3d": 1.25,
          "2d": 1.25,
          "1d": 1.25,
          "at": 1.25
        },
        "url": "https://arxiv.org/abs/2505.21444v1",
        "added_on": 1748414929,
        "published_on": 1748366160,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=ttuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=ttuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=ttuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177464844"
      },
      "priority_score": 29.89747196875
    },
    {
      "arxiv_id": "2505.21502",
      "title": "Generalizable and Relightable Gaussian Splatting for Human Novel View\n  Synthesis",
      "authors": [
        "Yipengjing Sun",
        "Chenyang Wang",
        "Shunyuan Zheng",
        "Zonglin Li",
        "Shengping Zhang",
        "Xiangyang Ji"
      ],
      "abstract": "We propose GRGS, a generalizable and relightable 3D Gaussian framework for\nhigh-fidelity human novel view synthesis under diverse lighting conditions.\nUnlike existing methods that rely on per-character optimization or ignore\nphysical constraints, GRGS adopts a feed-forward, fully supervised strategy\nthat projects geometry, material, and illumination cues from multi-view 2D\nobservations into 3D Gaussian representations. Specifically, to reconstruct\nlighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement\n(LGR) module trained on synthetically relit data to predict accurate depth and\nsurface normals. Based on the high-quality geometry, a Physically Grounded\nNeural Rendering (PGNR) module is further proposed to integrate neural\nprediction with physics-based shading, supporting editable relighting with\nshadows and indirect illumination. Besides, we design a 2D-to-3D projection\ntraining scheme that leverages differentiable supervision from ambient\nocclusion, direct, and indirect lighting maps, which alleviates the\ncomputational cost of explicit ray tracing. Extensive experiments demonstrate\nthat GRGS achieves superior visual quality, geometric consistency, and\ngeneralization across characters and lighting conditions.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-05-27T17:59:47+00:00",
      "updated": "2025-05-27T17:59:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21502v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21502v1",
      "doi": null,
      "altmetric_score": 1.25,
      "altmetric_data": {
        "title": "Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis",
        "arxiv_id": "2505.21502",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 4
        },
        "context": {
          "all": {
            "count": 28357944,
            "mean": 10.858551734286523,
            "rank": 18968177,
            "pct": 31,
            "higher_than": 8846410
          },
          "journal": {
            "count": 1154716,
            "mean": 3.9725805929769753,
            "rank": 362802,
            "pct": 63,
            "higher_than": 731375
          },
          "similar_age_3m": {
            "count": 143563,
            "mean": 9.759493421006804,
            "rank": 65117,
            "pct": 49,
            "higher_than": 71399
          },
          "similar_age_journal_3m": {
            "count": 33255,
            "mean": 2.0196218914448956,
            "rank": 8958,
            "pct": 62,
            "higher_than": 20823
          }
        },
        "authors": [
          "Yipengjing Sun",
          "Chenyang Wang",
          "Shunyuan Zheng",
          "Zonglin Li",
          "Shengping Zhang",
          "Xiangyang Ji"
        ],
        "type": "article",
        "pubdate": 1748368787,
        "altmetric_id": 177462961,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 2,
        "cited_by_posts_count": 10,
        "cited_by_tweeters_count": 2,
        "cited_by_accounts_count": 4,
        "last_updated": 1748434740,
        "score": 1.25,
        "history": {
          "1y": 1.25,
          "6m": 1.25,
          "3m": 1.25,
          "1m": 1.25,
          "1w": 1.25,
          "6d": 1.25,
          "5d": 1.25,
          "4d": 1.25,
          "3d": 1.25,
          "2d": 1.25,
          "1d": 1.25,
          "at": 1.25
        },
        "url": "https://arxiv.org/abs/2505.21502",
        "added_on": 1748406473,
        "published_on": 1748368787,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=2&types=tttuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=2&types=tttuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=2&types=tttuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177462961"
      },
      "priority_score": 29.762333079583335
    },
    {
      "arxiv_id": "2505.21499",
      "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery",
      "authors": [
        "Haowei Wang",
        "Junjie Wang",
        "Xiaojun Jia",
        "Rupeng Zhang",
        "Mingyang Li",
        "Zhe Liu",
        "Yang Liu",
        "Qing Wang"
      ],
      "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2025-05-27T17:59:05+00:00",
      "updated": "2025-05-27T17:59:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21499v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21499v1",
      "doi": null,
      "altmetric_score": 0.75,
      "altmetric_data": {
        "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
        "arxiv_id": "2505.21499",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 3
        },
        "context": {
          "all": {
            "count": 28355400,
            "mean": 10.859259586815917,
            "rank": 22404835,
            "pct": 18,
            "higher_than": 5181344
          },
          "journal": {
            "count": 1153541,
            "mean": 3.976851635095762,
            "rank": 515169,
            "pct": 44,
            "higher_than": 516615
          },
          "similar_age_3m": {
            "count": 141963,
            "mean": 9.805765023280713,
            "rank": 91920,
            "pct": 27,
            "higher_than": 39227
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 1.9987602152800747,
            "rank": 17506,
            "pct": 27,
            "higher_than": 9017
          }
        },
        "authors": [
          "Haowei Wang",
          "Junjie Wang",
          "Xiaojun Jia",
          "Rupeng Zhang",
          "Mingyang Li",
          "Zhe Liu",
          "Yang Liu",
          "Qing Wang"
        ],
        "type": "article",
        "pubdate": 1748368745,
        "altmetric_id": 177463784,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 3,
        "cited_by_posts_count": 11,
        "cited_by_accounts_count": 3,
        "last_updated": 1748413760,
        "score": 0.75,
        "history": {
          "1y": 0.75,
          "6m": 0.75,
          "3m": 0.75,
          "1m": 0.75,
          "1w": 0.75,
          "6d": 0.75,
          "5d": 0.75,
          "4d": 0.75,
          "3d": 0.75,
          "2d": 0.75,
          "1d": 0.75,
          "at": 0.75
        },
        "url": "https://arxiv.org/abs/2505.21499",
        "added_on": 1748410960,
        "published_on": 1748368745,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=1&types=uuuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=1&types=uuuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=1&types=uuuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463784"
      },
      "priority_score": 29.75649974597222
    },
    {
      "arxiv_id": "2505.21432",
      "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
      "authors": [
        "Haoming Song",
        "Delin Qu",
        "Yuanqi Yao",
        "Qizhi Chen",
        "Qi Lv",
        "Yiwen Tang",
        "Modi Shi",
        "Guanghui Ren",
        "Maoqing Yao",
        "Bin Zhao",
        "Dong Wang",
        "Xuelong Li"
      ],
      "abstract": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-05-27T17:04:21+00:00",
      "updated": "2025-05-27T17:04:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2505.21432v1.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21432v1",
      "doi": null,
      "altmetric_score": 0.75,
      "altmetric_data": {
        "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
        "arxiv_id": "2505.21432",
        "altmetric_jid": "arxiv",
        "journal": "arXiv",
        "cohorts": {
          "pub": 3
        },
        "context": {
          "all": {
            "count": 28355400,
            "mean": 10.859259586815917,
            "rank": 22404835,
            "pct": 18,
            "higher_than": 5181344
          },
          "journal": {
            "count": 1153541,
            "mean": 3.976851635095762,
            "rank": 515169,
            "pct": 44,
            "higher_than": 516615
          },
          "similar_age_3m": {
            "count": 141963,
            "mean": 9.805765023280713,
            "rank": 91920,
            "pct": 27,
            "higher_than": 39227
          },
          "similar_age_journal_3m": {
            "count": 33259,
            "mean": 1.996841937520671,
            "rank": 17440,
            "pct": 27,
            "higher_than": 9083
          }
        },
        "authors": [
          "Haoming Song",
          "Delin Qu",
          "Yuanqi Yao",
          "Qizhi Chen",
          "Qi Lv",
          "Yiwen Tang",
          "Modi Shi",
          "Guanghui Ren",
          "Maoqing Yao",
          "Bin Zhao",
          "Dong Wang",
          "Xuelong Li"
        ],
        "type": "article",
        "pubdate": 1748365461,
        "altmetric_id": 177463159,
        "schema": "1.5.4",
        "is_oa": false,
        "cited_by_bluesky_count": 3,
        "cited_by_posts_count": 10,
        "cited_by_accounts_count": 3,
        "last_updated": 1748413706,
        "score": 0.75,
        "history": {
          "1y": 0.75,
          "6m": 0.75,
          "3m": 0.75,
          "1m": 0.75,
          "1w": 0.75,
          "6d": 0.75,
          "5d": 0.75,
          "4d": 0.75,
          "3d": 0.75,
          "2d": 0.75,
          "1d": 0.75,
          "at": 0.75
        },
        "url": "https://arxiv.org/abs/2505.21432",
        "added_on": 1748406867,
        "published_on": 1748365461,
        "readers": {
          "citeulike": 0,
          "mendeley": 0,
          "connotea": 0
        },
        "readers_count": 0,
        "downloads": [],
        "images": {
          "small": "https://badges.altmetric.com/?size=64&score=1&types=uuuuuuuu",
          "medium": "https://badges.altmetric.com/?size=100&score=1&types=uuuuuuuu",
          "large": "https://badges.altmetric.com/?size=180&score=1&types=uuuuuuuu"
        },
        "details_url": "https://www.altmetric.com/details.php?citation_id=177463159"
      },
      "priority_score": 29.300388634583335
    }
  ],
  "last_reset": "2025-05-28",
  "last_discovery": "2025-05-28T18:34:49.873002+00:00",
  "last_posting": "2025-05-28T18:36:01.148360+00:00"
}